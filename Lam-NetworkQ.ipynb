{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNsBPn1h7SlofQxkbhh1Y06"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bTfAoa30ejjX","executionInfo":{"status":"ok","timestamp":1725537647869,"user_tz":-60,"elapsed":15162,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Define the states and actions\n","states = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n","numStates = len(states)\n","numActions = 4  # ['up', 'down', 'right', 'left']\n","\n","# Define the transition matrix\n","R = np.array([\n","    ['A', 'E', 'B', 'A'],\n","    ['B', 'F', 'C', 'A'],\n","    ['C', 'G', 'D', 'B'],\n","    ['D', 'H', 'D', 'C'],\n","    ['A', 'I', 'F', 'E'],\n","    ['B', 'J', 'G', 'E'],\n","    ['C', 'K', 'H', 'F'],\n","    ['D', 'L', 'H', 'G'],\n","    ['E', 'M', 'J', 'I'],\n","    ['F', 'N', 'K', 'I'],\n","    ['G', 'O', 'L', 'J'],\n","    ['H', 'P', 'L', 'K'],\n","    ['I', 'M', 'N', 'M'],\n","    ['J', 'N', 'O', 'M'],\n","    ['K', 'O', 'P', 'N'],\n","    ['L', 'P', 'P', 'O']\n","])\n","\n","listOfHoles = np.array(['F', 'H', 'L', 'M'])\n","\n","# Function to choose an action using epsilon-greedy policy\n","def choose_action(state_vector):\n","    if np.random.uniform(0, 1) < epsilon:\n","        return np.random.randint(0, numActions)  # Explore\n","    else:\n","        q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","        return np.argmax(q_values)  # Exploit\n","\n","# Convert states to one-hot encoded vectors, ie: state A: (1, 0, 0, 0, 0, ... 0)\n","def state_to_one_hot(state):\n","    one_hot = np.zeros(numStates)\n","    one_hot[states.index(state)] = 1\n","    return one_hot"]},{"cell_type":"code","source":["inputs = tf.keras.Input(shape=(numStates,))\n","x = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n","x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n","outputs = tf.keras.layers.Dense(numActions, activation=\"linear\")(x)\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')  # Using Adam optimizer with a learning rate and Mean Squared Error loss"],"metadata":{"id":"MDHK8kx0ezjQ","executionInfo":{"status":"ok","timestamp":1725537648545,"user_tz":-60,"elapsed":677,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","gamma = 0.9    # Discount factor\n","maxSteps = 99  # Maximum steps per episode\n","epsilon_decay = 0.995  # Epsilon decay rate\n","epsilon_min = 0.01  # Minimum epsilon value\n","epsilon = 1.0  # Exploration rate\n","\n","# Initialize the Q-table with zeros\n","q_table = np.zeros((numStates, numActions))\n","\n","\n","# Q-network training algorithm\n","for episode in range(500):\n","    state = 'A'  # Start from state A\n","    state_index = states.index(state)\n","    state_vector = state_to_one_hot(state)  # One-hot encoded state vector\n","\n","    total_reward = 0\n","\n","    for step in range(maxSteps):\n","        action = choose_action(state_vector)\n","\n","        next_state = R[state_index, action]\n","        next_state_index = states.index(next_state)\n","        next_state_vector = state_to_one_hot(next_state)\n","\n","        reward = 0\n","        if next_state == 'P':\n","            reward = 1\n","        if next_state in listOfHoles:\n","            reward = -1\n","\n","        # Predict Q-values for the current state\n","        q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","\n","        # Predict Q-values for the next state\n","        q_values_next = model.predict(next_state_vector.reshape(1, -1), verbose=0)\n","\n","        # Compute target Q-value\n","        target_q_value = reward + gamma * np.max(q_values_next)\n","        q_values[0][action] = target_q_value\n","\n","        # Update the Q-network\n","        model.fit(state_vector.reshape(1, -1), q_values, epochs=1, verbose=0)\n","\n","        # Update the Q-table\n","        q_table[state_index, action] = q_values[0, action]\n","\n","        # Transition to the next state\n","        state = next_state\n","        state_index = next_state_index\n","        state_vector = next_state_vector\n","        total_reward += reward\n","\n","        # End the episode if the goal state or a terminal state is reached\n","        if state == 'P':\n","            break\n","\n","    # Decay epsilon\n","    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    # Print progress\n","    if (episode % 100 == 0):\n","      print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n","\n","print(\"Training finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvOWawvQf1GA","executionInfo":{"status":"ok","timestamp":1725540323552,"user_tz":-60,"elapsed":2112907,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"fd702d8c-f7e4-46db-e99c-168bf59cbcd8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1, Total Reward: 0, Epsilon: 0.9950\n","Episode 101, Total Reward: -5, Epsilon: 0.6027\n","Episode 201, Total Reward: 0, Epsilon: 0.3651\n","Episode 301, Total Reward: 1, Epsilon: 0.2212\n","Episode 401, Total Reward: 1, Epsilon: 0.1340\n","Training finished.\n"]}]},{"cell_type":"code","source":["#q_table = np.zeros((numStates, numActions))\n","for i, state in enumerate(states):\n","    print(f\"State: {state}\")\n","    # Convert state to one-hot encoded vector\n","    #state_vector = np.zeros(numStates)\n","    #state_vector[i] = 1  # One-hot encode the state\n","    #print(state_vector)\n","    state_vector = state_to_one_hot(state)\n","\n","    # Predict Q-values for this state\n","    q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","\n","    # Store Q-values in the Q-table\n","    print(q_values)\n","    #q_table[i] = q_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFHEem_lhBHI","executionInfo":{"status":"ok","timestamp":1725540324840,"user_tz":-60,"elapsed":1289,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"547937db-9d06-4869-9572-71fa60ed83fe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["State: A\n","[[1.3328207 1.51126   1.5010242 1.3186691]]\n","State: B\n","[[1.4500897 0.7682275 1.7311196 1.3010823]]\n","State: C\n","[[1.5903533 1.8798285 1.6465975 1.4859785]]\n","State: D\n","[[1.6033435  0.89933145 1.5336784  1.604383  ]]\n","State: E\n","[[1.3380239  1.6823069  0.74121207 1.4308952 ]]\n","State: F\n","[[1.4216535 1.9550904 1.7443982 1.3938609]]\n","State: G\n","[[1.514033   2.1128945  0.4778389  0.56738067]]\n","State: H\n","[[1.5082058 1.3433236 0.7977875 1.8707659]]\n","State: I\n","[[1.447344  0.975004  1.8684891 1.5659631]]\n","State: J\n","[[0.5895027 2.0089505 2.0927844 1.6538521]]\n","State: K\n","[[1.7925892 2.330699  1.3492628 1.8088256]]\n","State: L\n","[[0.69394743 2.5482488  1.35448    2.0937092 ]]\n","State: M\n","[[1.5784128 0.9652873 2.087463  0.4698567]]\n","State: N\n","[[1.7815741 2.0534625 2.2903192 0.7907319]]\n","State: O\n","[[2.0120642 2.4000647 2.5784242 2.1199555]]\n","State: P\n","[[1.0648654 1.7598311 1.3366537 1.196108 ]]\n"]}]},{"cell_type":"code","source":["# After training, you can inspect the Q-table\n","print(\"Final Q-table values:\")\n","print(q_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ov87cf6vT1x","executionInfo":{"status":"ok","timestamp":1725540324841,"user_tz":-60,"elapsed":3,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"4ed77648-3705-4c45-a26b-59e9e0b4dacb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Q-table values:\n","[[1.41292155 1.51252019 1.51933265 1.41727674]\n"," [1.498631   0.74103463 1.7395221  1.33524847]\n"," [1.67111039 1.91287959 1.68872178 1.50105464]\n"," [1.69091606 0.82301164 1.55668736 1.69176376]\n"," [1.43605626 1.67859304 0.74833387 1.56016254]\n"," [1.49293113 1.86220431 1.79138494 1.50683486]\n"," [1.67500031 2.08572602 0.5446021  0.61860746]\n"," [1.60990405 1.32321012 0.76452166 1.79462039]\n"," [1.51083755 0.91600746 1.88782454 1.75934124]\n"," [0.49884856 2.11750293 2.0989356  1.70609534]\n"," [1.85334396 2.33225703 1.28151917 1.88663983]\n"," [0.78992724 2.57954025 1.37964344 2.15104461]\n"," [1.68263197 0.90771687 2.09026408 0.81890637]\n"," [2.00242043 2.18334699 2.35368085 0.89215791]\n"," [2.18191814 2.43542695 2.58343816 2.12376976]\n"," [0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"markdown","source":["# Testing with Model.Numpy()"],"metadata":{"id":"sXS0YSX7OCTn"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","# Define the states and actions\n","states = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n","numStates = len(states)\n","numActions = 4  # ['up', 'down', 'right', 'left']\n","\n","# Define the transition matrix\n","R = np.array([\n","    ['A', 'E', 'B', 'A'],\n","    ['B', 'F', 'C', 'A'],\n","    ['C', 'G', 'D', 'B'],\n","    ['D', 'H', 'D', 'C'],\n","    ['A', 'I', 'F', 'E'],\n","    ['B', 'J', 'G', 'E'],\n","    ['C', 'K', 'H', 'F'],\n","    ['D', 'L', 'H', 'G'],\n","    ['E', 'M', 'J', 'I'],\n","    ['F', 'N', 'K', 'I'],\n","    ['G', 'O', 'L', 'J'],\n","    ['H', 'P', 'L', 'K'],\n","    ['I', 'M', 'N', 'M'],\n","    ['J', 'N', 'O', 'M'],\n","    ['K', 'O', 'P', 'N'],\n","    ['L', 'P', 'P', 'O']\n","])\n","\n","listOfHoles = np.array(['F', 'H', 'L', 'M'])\n","\n","# Function to choose an action using epsilon-greedy policy\n","def choose_action(state_vector):\n","    if np.random.uniform(0, 1) < epsilon:\n","        return np.random.randint(0, numActions)  # Explore\n","    else:\n","        q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","        return np.argmax(q_values)  # Exploit\n","\n","# Convert states to one-hot encoded vectors, ie: state A: (1, 0, 0, 0, 0, ... 0)\n","def state_to_one_hot(state):\n","    one_hot = np.zeros(numStates)\n","    one_hot[states.index(state)] = 1\n","    return one_hot"],"metadata":{"id":"X7A-gmKnOFhW","executionInfo":{"status":"ok","timestamp":1725540495022,"user_tz":-60,"elapsed":2,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["inputs = tf.keras.Input(shape=(numStates,))\n","x = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n","x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n","outputs = tf.keras.layers.Dense(numActions, activation=\"linear\")(x)\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')  # Using Adam optimizer with a learning rate and Mean Squared Error loss"],"metadata":{"id":"d5s86v4bOMIi","executionInfo":{"status":"ok","timestamp":1725540496769,"user_tz":-60,"elapsed":204,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","gamma = 0.9    # Discount factor\n","maxSteps = 99  # Maximum steps per episode\n","epsilon_decay = 0.995  # Epsilon decay rate\n","epsilon_min = 0.01  # Minimum epsilon value\n","epsilon = 1.0  # Exploration rate\n","\n","# Initialize the Q-table with zeros\n","q_table = np.zeros((numStates, numActions))\n","\n","\n","# Q-network training algorithm\n","for episode in range(500):\n","    state = 'A'  # Start from state A\n","    state_index = states.index(state)\n","    state_vector = state_to_one_hot(state)  # One-hot encoded state vector\n","\n","    total_reward = 0\n","\n","    for step in range(maxSteps):\n","        action = choose_action(state_vector)\n","\n","        next_state = R[state_index, action]\n","        next_state_index = states.index(next_state)\n","        next_state_vector = state_to_one_hot(next_state)\n","\n","        reward = 0\n","        if next_state == 'P':\n","            reward = 1\n","        if next_state in listOfHoles:\n","            reward = -1\n","\n","        # Predict Q-values for the current state\n","        #q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","\n","        # Predict Q-values for the next state\n","        #q_values_next = model.predict(next_state_vector.reshape(1, -1), verbose=0)\n","        q_values = model(state_vector.reshape(1,-1)).numpy()\n","        q_values_next = model(next_state_vector.reshape(1,-1)).numpy()\n","\n","        # Compute target Q-value\n","        target_q_value = reward + gamma * np.max(q_values_next)\n","        q_values[0][action] = target_q_value\n","\n","        # Update the Q-network\n","        model.fit(state_vector.reshape(1, -1), q_values, epochs=1, verbose=0)\n","\n","        # Update the Q-table\n","        q_table[state_index, action] = q_values[0, action]\n","\n","        # Transition to the next state\n","        state = next_state\n","        state_index = next_state_index\n","        state_vector = next_state_vector\n","        total_reward += reward\n","\n","        # End the episode if the goal state or a terminal state is reached\n","        if state == 'P':\n","            break\n","\n","    # Decay epsilon\n","    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    # Print progress\n","    if (episode % 100 == 0):\n","      print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n","\n","print(\"Training finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcZ5idXIOMhp","executionInfo":{"status":"ok","timestamp":1725541430390,"user_tz":-60,"elapsed":885174,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"b6d02fc5-82d2-4eb0-a3a9-74d963a4dd8d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1, Total Reward: -4, Epsilon: 0.9950\n","Episode 101, Total Reward: -3, Epsilon: 0.6027\n","Episode 201, Total Reward: 0, Epsilon: 0.3651\n","Episode 301, Total Reward: 1, Epsilon: 0.2212\n","Episode 401, Total Reward: 1, Epsilon: 0.1340\n","Training finished.\n"]}]},{"cell_type":"code","source":["#q_table = np.zeros((numStates, numActions))\n","for i, state in enumerate(states):\n","    print(f\"State: {state}\")\n","    # Convert state to one-hot encoded vector\n","    #state_vector = np.zeros(numStates)\n","    #state_vector[i] = 1  # One-hot encode the state\n","    #print(state_vector)\n","    state_vector = state_to_one_hot(state)\n","\n","    # Predict Q-values for this state\n","    q_values = model.predict(state_vector.reshape(1, -1), verbose=0)\n","\n","    # Store Q-values in the Q-table\n","    print(q_values)\n","    #q_table[i] = q_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAkvkc87O3l2","executionInfo":{"status":"ok","timestamp":1725541431808,"user_tz":-60,"elapsed":1420,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"8a06f946-aefe-48f1-9be6-7439c07ffdde"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["State: A\n","[[1.1262012 1.3194517 1.2075603 1.1374321]]\n","State: B\n","[[1.0744698  0.48568296 1.4033614  1.0688598 ]]\n","State: C\n","[[1.2371029 1.6719157 1.3061504 1.3156047]]\n","State: D\n","[[1.1751027 0.5113915 1.1944841 1.4433489]]\n","State: E\n","[[1.1287448  1.4483818  0.46822304 1.2560023 ]]\n","State: F\n","[[1.2923993 1.585337  1.6055459 1.1033381]]\n","State: G\n","[[1.3204914  1.7781868  0.29287404 0.47019634]]\n","State: H\n","[[1.2303267 1.0807898 0.4837227 1.4578196]]\n","State: I\n","[[1.3076162 0.6445928 1.6022233 1.4844861]]\n","State: J\n","[[0.46431753 1.7911094  1.848902   1.4417734 ]]\n","State: K\n","[[1.5465803  2.0447042  0.98030543 1.5636303 ]]\n","State: L\n","[[0.48448977 2.2461298  0.9532682  1.6939882 ]]\n","State: M\n","[[1.5111339  0.60150796 1.7242556  0.60496473]]\n","State: N\n","[[1.5779389  1.755153   2.0412478  0.51644015]]\n","State: O\n","[[1.7569519 1.9751118 2.2454386 1.7070776]]\n","State: P\n","[[0.6739152 1.3724157 1.0600637 1.0609677]]\n"]}]},{"cell_type":"code","source":["# After training, you can inspect the Q-table\n","print(\"Final Q-table values:\")\n","print(q_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Z_UdfUjO-Qm","executionInfo":{"status":"ok","timestamp":1725541431808,"user_tz":-60,"elapsed":3,"user":{"displayName":"Lam Ngo","userId":"05135763133071598981"}},"outputId":"95940e10-5f24-4adb-840b-31941111b88a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Q-table values:\n","[[1.15917659 1.31634831 1.18085051 1.19001567]\n"," [1.15775573 0.45434201 1.3830086  1.18859851]\n"," [1.39777339 1.55836093 1.35365069 1.40368533]\n"," [1.24285328 0.44459453 1.2852664  1.48885334]\n"," [1.13642538 1.41032875 0.46954349 1.2989732 ]\n"," [1.32950115 1.63182855 1.61502826 1.19101334]\n"," [1.49608231 1.69825029 0.31144345 0.39495715]\n"," [1.35865843 0.99079388 0.40586913 1.47154796]\n"," [1.32184458 0.60135967 1.61806345 1.47508609]\n"," [0.45227489 1.83075523 1.81698179 1.47020042]\n"," [1.63840806 2.01182604 0.87376481 1.64678192]\n"," [0.386186   2.15031147 0.87297261 1.77129114]\n"," [1.50512099 0.63633609 1.74565935 0.59004593]\n"," [1.61184669 1.83036625 2.02066994 0.60559529]\n"," [1.79670632 2.01953459 2.23034763 1.80333149]\n"," [0.         0.         0.         0.        ]]\n"]}]}]}